import os
import chromadb
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader
import hashlib

# ==========================================
# [è¨­å®šå€]
# ==========================================
DB_PATH = "./chroma_db"  # è³‡æ–™åº«å­˜åœ¨æœ¬åœ°ï¼Œå¯¦ç¾æŒä¹…åŒ–
COLLECTION_NAME = "blockchain_knowledge"
DOC_FOLDER = "documents"

class BlockchainRAG:
    def __init__(self):
        print("ğŸ“š [RAG] åˆå§‹åŒ–çŸ¥è­˜åº«å¼•æ“...")
        
        # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (ä½¿ç”¨è¼•é‡ç´šæ¨¡å‹)
        # 'all-MiniLM-L6-v2' é€Ÿåº¦å¿«ï¼Œæ”¯æ´ä¸­è‹±æ–‡ï¼Œéå¸¸é©åˆå°ˆé¡Œ
        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # 2. åˆå§‹åŒ– ChromaDB (PersistentClient æœƒæŠŠè³‡æ–™å­˜åˆ°ç¡¬ç¢Ÿ)
        self.client = chromadb.PersistentClient(path=DB_PATH)
        
        # å–å¾—æˆ–å»ºç«‹ Collection
        self.collection = self.client.get_or_create_collection(name=COLLECTION_NAME)
        
        # 3. è‡ªå‹•æª¢æŸ¥ä¸¦è¼‰å…¥æ–‡ä»¶
        self._ingest_documents()

    def _ingest_documents(self):
        """
        è®€å– documents è³‡æ–™å¤¾ï¼Œå°‡æ–°æª”æ¡ˆåˆ‡å¡Šä¸¦å­˜å…¥å‘é‡è³‡æ–™åº«
        """
        if not os.path.exists(DOC_FOLDER):
            os.makedirs(DOC_FOLDER)
            print(f"âš ï¸ å»ºç«‹ {DOC_FOLDER} è³‡æ–™å¤¾ï¼Œè«‹æ”¾å…¥ PDF æˆ– TXT æ–‡ä»¶ã€‚")
            return

        # å–å¾—ç›®å‰ DB è£¡å·²ç¶“æœ‰çš„æª”æ¡ˆ ID (é¿å…é‡è¤‡è®€å–)
        existing_ids = self.collection.get()['ids']
        existing_hashes = set([id.split('_')[0] for id in existing_ids]) # ç°¡å–®ç”¨æª”åhashåˆ¤æ–·

        print(f"ğŸ“‚ [RAG] æƒææ–‡ä»¶è³‡æ–™å¤¾: {DOC_FOLDER}")
        
        for filename in os.listdir(DOC_FOLDER):
            file_path = os.path.join(DOC_FOLDER, filename)
            
            # ç”¢ç”Ÿç°¡å–®çš„ file hash (é€™è£¡ç”¨æª”åä»£æ›¿ï¼Œæ­£å¼å°ˆæ¡ˆå¯ç”¨ content hash)
            file_hash = hashlib.md5(filename.encode()).hexdigest()
            
            if file_hash in existing_hashes:
                # print(f" - {filename} å·²å­˜åœ¨ï¼Œè·³éã€‚")
                continue
            
            print(f"   ğŸ‘‰ ç™¼ç¾æ–°æ–‡ä»¶ï¼Œæ­£åœ¨è™•ç†: {filename} ...")
            
            # è®€å–æ–‡å­—å…§å®¹
            text_content = ""
            if filename.endswith(".pdf"):
                text_content = self._read_pdf(file_path)
            elif filename.endswith(".txt"):
                with open(file_path, "r", encoding="utf-8") as f:
                    text_content = f.read()
            else:
                continue # è·³éä¸æ”¯æ´çš„æ ¼å¼

            if not text_content: continue

            # åˆ‡å¡Š (Chunking)
            chunks = self._chunk_text(text_content, chunk_size=400, overlap=50)
            
            # æº–å‚™å¯«å…¥ DB
            ids = [f"{file_hash}_{i}" for i in range(len(chunks))]
            metadatas = [{"source": filename, "chunk_id": i} for i in range(len(chunks))]
            embeddings = self.embed_model.encode(chunks).tolist()

            self.collection.add(
                ids=ids,
                documents=chunks,
                embeddings=embeddings,
                metadatas=metadatas
            )
            print(f"   âœ… å·²å­˜å…¥ {len(chunks)} å€‹ç‰‡æ®µã€‚")

    def _read_pdf(self, path):
        """è®€å– PDF æ–‡å­—"""
        try:
            reader = PdfReader(path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            print(f"âŒ PDF è®€å–å¤±æ•— {path}: {e}")
            return ""

    def _chunk_text(self, text, chunk_size=400, overlap=50):
        """
        ç°¡å–®çš„æ»‘å‹•è¦–çª—åˆ‡å¡Š (Sliding Window Chunking)
        """
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = start + chunk_size
            chunk = text[start:end]
            
            # ç°¡å–®æ¸…ç†æ›è¡Œç¬¦è™Ÿï¼Œè®“èªæ„æ›´é€£è²«
            chunk = chunk.replace('\n', ' ')
            chunks.append(chunk)
            
            # ç§»å‹•è¦–çª— (é‡ç–Š overlap)
            start += (chunk_size - overlap)
            
        return chunks

    def search(self, query, top_k=3):
        """
        æ ¸å¿ƒåŠŸèƒ½ï¼šçµ¦å®šå•é¡Œï¼Œæ‰¾å›æœ€ç›¸é—œçš„ k å€‹ç‰‡æ®µ
        """
        query_vec = self.embed_model.encode([query]).tolist()
        
        results = self.collection.query(
            query_embeddings=query_vec,
            n_results=top_k
        )
        
        # æ•´ç†å›å‚³æ ¼å¼
        retrieved_data = []
        if results['documents']:
            for i in range(len(results['documents'][0])):
                doc = results['documents'][0][i]
                meta = results['metadatas'][0][i]
                retrieved_data.append(f"ã€ä¾†æº: {meta['source']}ã€‘\n{doc}")
                
        return "\n\n".join(retrieved_data)

# æ¸¬è©¦ç”¨
if __name__ == "__main__":
    rag = BlockchainRAG()
    # æ¸¬è©¦æœå°‹
    print("\nğŸ” æ¸¬è©¦æœå°‹: 'æ¯”ç‰¹å¹£çš„é‹ä½œåŸç†'")
    print(rag.search("æ¯”ç‰¹å¹£çš„é‹ä½œåŸç†", top_k=2))